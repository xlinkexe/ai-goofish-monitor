import asyncio
import sys
import os
import argparse
import math
import json
import random
import base64
import re
import time
from datetime import datetime
from functools import wraps
from urllib.parse import urlencode

import requests
from dotenv import load_dotenv
from openai import AsyncOpenAI, APIStatusError
from playwright.async_api import async_playwright, Response, TimeoutError as PlaywrightTimeoutError
from requests.exceptions import HTTPError

# å®šä¹‰ç™»å½•çŠ¶æ€æ–‡ä»¶çš„è·¯å¾„
STATE_FILE = "xianyu_state.json"
# å®šä¹‰é—²é±¼æœç´¢APIçš„URLç‰¹å¾
API_URL_PATTERN = "h5api.m.goofish.com/h5/mtop.taobao.idlemtopsearch.pc.search"
# å®šä¹‰é—²é±¼è¯¦æƒ…é¡µAPIçš„URLç‰¹å¾
DETAIL_API_URL_PATTERN = "h5api.m.goofish.com/h5/mtop.taobao.idle.pc.detail"

# --- AI & Notification Configuration ---
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL")
MODEL_NAME = os.getenv("OPENAI_MODEL_NAME")
NTFY_TOPIC_URL = os.getenv("NTFY_TOPIC_URL")
WX_BOT_URL = os.getenv("WX_BOT_URL")
PCURL_TO_MOBILE = os.getenv("PCURL_TO_MOBILE")
RUN_HEADLESS = os.getenv("RUN_HEADLESS", "true").lower() != "false"

# æ£€æŸ¥é…ç½®æ˜¯å¦é½å…¨
if not all([BASE_URL, MODEL_NAME]):
    sys.exit("é”™è¯¯ï¼šè¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­å®Œæ•´è®¾ç½®äº† OPENAI_BASE_URL å’Œ OPENAI_MODEL_NAMEã€‚(OPENAI_API_KEY å¯¹äºæŸäº›æœåŠ¡æ˜¯å¯é€‰çš„)")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
try:
    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
except Exception as e:
    sys.exit(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")

# å®šä¹‰ç›®å½•å’Œæ–‡ä»¶å
IMAGE_SAVE_DIR = "images"
os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)

# å®šä¹‰ä¸‹è½½å›¾ç‰‡æ‰€éœ€çš„è¯·æ±‚å¤´
IMAGE_DOWNLOAD_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:139.0) Gecko/20100101 Firefox/139.0',
    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def convert_goofish_link(url: str) -> str:
    """
    å°†Goofishå•†å“é“¾æ¥è½¬æ¢ä¸ºåªåŒ…å«å•†å“IDçš„æ‰‹æœºç«¯æ ¼å¼ã€‚

    Args:
        url: åŸå§‹çš„Goofishå•†å“é“¾æ¥ã€‚

    Returns:
        è½¬æ¢åçš„ç®€æ´é“¾æ¥ï¼Œæˆ–åœ¨æ— æ³•è§£ææ—¶è¿”å›åŸå§‹é“¾æ¥ã€‚
    """
    # åŒ¹é…ç¬¬ä¸€ä¸ªé“¾æ¥ä¸­çš„å•†å“IDæ¨¡å¼ï¼šitem?id= åé¢çš„æ•°å­—ä¸²
    match_first_link = re.search(r'item\?id=(\d+)', url)
    if match_first_link:
        item_id = match_first_link.group(1)
        return f"https://pages.goofish.com/sharexy?loadingVisible=false&bft=item&bfs=idlepc.item&spm=a21ybx.item.0.0&bfp={{\"id\":{item_id}}}"

    return url

def get_link_unique_key(link: str) -> str:
    """æˆªå–é“¾æ¥ä¸­ç¬¬ä¸€ä¸ª"&"ä¹‹å‰çš„å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ä¾æ®ã€‚"""
    return link.split('&', 1)[0]

async def random_sleep(min_seconds: float, max_seconds: float):
    """å¼‚æ­¥ç­‰å¾…ä¸€ä¸ªåœ¨æŒ‡å®šèŒƒå›´å†…çš„éšæœºæ—¶é—´ã€‚"""
    delay = random.uniform(min_seconds, max_seconds)
    print(f"   [å»¶è¿Ÿ] ç­‰å¾… {delay:.2f} ç§’... (èŒƒå›´: {min_seconds}-{max_seconds}s)") # è°ƒè¯•æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Š
    await asyncio.sleep(delay)

async def save_to_jsonl(data_record: dict, keyword: str):
    """å°†ä¸€ä¸ªåŒ…å«å•†å“å’Œå–å®¶ä¿¡æ¯çš„å®Œæ•´è®°å½•è¿½åŠ ä¿å­˜åˆ° .jsonl æ–‡ä»¶ã€‚"""
    filename = f"{keyword.replace(' ', '_')}_full_data.jsonl"
    try:
        with open(filename, "a", encoding="utf-8") as f:
            f.write(json.dumps(data_record, ensure_ascii=False) + "\n")
        return True
    except IOError as e:
        print(f"å†™å…¥æ–‡ä»¶ {filename} å‡ºé”™: {e}")
        return False

async def calculate_reputation_from_ratings(ratings_json: list) -> dict:
    """ä»åŸå§‹è¯„ä»·APIæ•°æ®åˆ—è¡¨ä¸­ï¼Œè®¡ç®—ä½œä¸ºå–å®¶å’Œä¹°å®¶çš„å¥½è¯„æ•°ä¸å¥½è¯„ç‡ã€‚"""
    seller_total = 0
    seller_positive = 0
    buyer_total = 0
    buyer_positive = 0

    for card in ratings_json:
        # ä½¿ç”¨ safe_get ä¿è¯å®‰å…¨è®¿é—®
        data = await safe_get(card, 'cardData', default={})
        role_tag = await safe_get(data, 'rateTagList', 0, 'text', default='')
        rate_type = await safe_get(data, 'rate') # 1=å¥½è¯„, 0=ä¸­è¯„, -1=å·®è¯„

        if "å–å®¶" in role_tag:
            seller_total += 1
            if rate_type == 1:
                seller_positive += 1
        elif "ä¹°å®¶" in role_tag:
            buyer_total += 1
            if rate_type == 1:
                buyer_positive += 1

    # è®¡ç®—æ¯”ç‡ï¼Œå¹¶å¤„ç†é™¤ä»¥é›¶çš„æƒ…å†µ
    seller_rate = f"{(seller_positive / seller_total * 100):.2f}%" if seller_total > 0 else "N/A"
    buyer_rate = f"{(buyer_positive / buyer_total * 100):.2f}%" if buyer_total > 0 else "N/A"

    return {
        "ä½œä¸ºå–å®¶çš„å¥½è¯„æ•°": f"{seller_positive}/{seller_total}",
        "ä½œä¸ºå–å®¶çš„å¥½è¯„ç‡": seller_rate,
        "ä½œä¸ºä¹°å®¶çš„å¥½è¯„æ•°": f"{buyer_positive}/{buyer_total}",
        "ä½œä¸ºä¹°å®¶çš„å¥½è¯„ç‡": buyer_rate
    }

async def _parse_user_items_data(items_json: list) -> list:
    """è§£æç”¨æˆ·ä¸»é¡µçš„å•†å“åˆ—è¡¨APIçš„JSONæ•°æ®ã€‚"""
    parsed_list = []
    for card in items_json:
        data = card.get('cardData', {})
        status_code = data.get('itemStatus')
        if status_code == 0:
            status_text = "åœ¨å”®"
        elif status_code == 1:
            status_text = "å·²å”®"
        else:
            status_text = f"æœªçŸ¥çŠ¶æ€ ({status_code})"

        parsed_list.append({
            "å•†å“ID": data.get('id'),
            "å•†å“æ ‡é¢˜": data.get('title'),
            "å•†å“ä»·æ ¼": data.get('priceInfo', {}).get('price'),
            "å•†å“ä¸»å›¾": data.get('picInfo', {}).get('picUrl'),
            "å•†å“çŠ¶æ€": status_text
        })
    return parsed_list


async def scrape_user_profile(context, user_id: str) -> dict:
    """
    ã€æ–°ç‰ˆã€‘è®¿é—®æŒ‡å®šç”¨æˆ·çš„ä¸ªäººä¸»é¡µï¼ŒæŒ‰é¡ºåºé‡‡é›†å…¶æ‘˜è¦ä¿¡æ¯ã€å®Œæ•´çš„å•†å“åˆ—è¡¨å’Œå®Œæ•´çš„è¯„ä»·åˆ—è¡¨ã€‚
    """
    print(f"   -> å¼€å§‹é‡‡é›†ç”¨æˆ·ID: {user_id} çš„å®Œæ•´ä¿¡æ¯...")
    profile_data = {}
    page = await context.new_page()

    # ä¸ºå„é¡¹å¼‚æ­¥ä»»åŠ¡å‡†å¤‡Futureå’Œæ•°æ®å®¹å™¨
    head_api_future = asyncio.get_event_loop().create_future()

    all_items, all_ratings = [], []
    stop_item_scrolling, stop_rating_scrolling = asyncio.Event(), asyncio.Event()

    async def handle_response(response: Response):
        # æ•è·å¤´éƒ¨æ‘˜è¦API
        if "mtop.idle.web.user.page.head" in response.url and not head_api_future.done():
            try:
                head_api_future.set_result(await response.json())
                print(f"      [APIæ•è·] ç”¨æˆ·å¤´éƒ¨ä¿¡æ¯... æˆåŠŸ")
            except Exception as e:
                if not head_api_future.done(): head_api_future.set_exception(e)

        # æ•è·å•†å“åˆ—è¡¨API
        elif "mtop.idle.web.xyh.item.list" in response.url:
            try:
                data = await response.json()
                all_items.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] å•†å“åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_items)} ä»¶")
                if not data.get('data', {}).get('nextPage', True):
                    stop_item_scrolling.set()
            except Exception as e:
                stop_item_scrolling.set()

        # æ•è·è¯„ä»·åˆ—è¡¨API
        elif "mtop.idle.web.trade.rate.list" in response.url:
            try:
                data = await response.json()
                all_ratings.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] è¯„ä»·åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_ratings)} æ¡")
                if not data.get('data', {}).get('nextPage', True):
                    stop_rating_scrolling.set()
            except Exception as e:
                stop_rating_scrolling.set()

    page.on("response", handle_response)

    try:
        # --- ä»»åŠ¡1: å¯¼èˆªå¹¶é‡‡é›†å¤´éƒ¨ä¿¡æ¯ ---
        await page.goto(f"https://www.goofish.com/personal?userId={user_id}", wait_until="domcontentloaded", timeout=20000)
        head_data = await asyncio.wait_for(head_api_future, timeout=15)
        profile_data = await parse_user_head_data(head_data)

        # --- ä»»åŠ¡2: æ»šåŠ¨åŠ è½½æ‰€æœ‰å•†å“ (é»˜è®¤é¡µé¢) ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„å•†å“åˆ—è¡¨...")
        await random_sleep(2, 4) # ç­‰å¾…ç¬¬ä¸€é¡µå•†å“APIå®Œæˆ
        while not stop_item_scrolling.is_set():
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            try:
                await asyncio.wait_for(stop_item_scrolling.wait(), timeout=8)
            except asyncio.TimeoutError:
                print("      [æ»šåŠ¨è¶…æ—¶] å•†å“åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                break
        profile_data["å–å®¶å‘å¸ƒçš„å•†å“åˆ—è¡¨"] = await _parse_user_items_data(all_items)

        # --- ä»»åŠ¡3: ç‚¹å‡»å¹¶é‡‡é›†æ‰€æœ‰è¯„ä»· ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„è¯„ä»·åˆ—è¡¨...")
        rating_tab_locator = page.locator("//div[text()='ä¿¡ç”¨åŠè¯„ä»·']/ancestor::li")
        if await rating_tab_locator.count() > 0:
            await rating_tab_locator.click()
            await random_sleep(3, 5) # ç­‰å¾…ç¬¬ä¸€é¡µè¯„ä»·APIå®Œæˆ

            while not stop_rating_scrolling.is_set():
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                try:
                    await asyncio.wait_for(stop_rating_scrolling.wait(), timeout=8)
                except asyncio.TimeoutError:
                    print("      [æ»šåŠ¨è¶…æ—¶] è¯„ä»·åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                    break

            profile_data['å–å®¶æ”¶åˆ°çš„è¯„ä»·åˆ—è¡¨'] = await parse_ratings_data(all_ratings)
            reputation_stats = await calculate_reputation_from_ratings(all_ratings)
            profile_data.update(reputation_stats)
        else:
            print("      [è­¦å‘Š] æœªæ‰¾åˆ°è¯„ä»·é€‰é¡¹å¡ï¼Œè·³è¿‡è¯„ä»·é‡‡é›†ã€‚")

    except Exception as e:
        print(f"   [é”™è¯¯] é‡‡é›†ç”¨æˆ· {user_id} ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        page.remove_listener("response", handle_response)
        await page.close()
        print(f"   -> ç”¨æˆ· {user_id} ä¿¡æ¯é‡‡é›†å®Œæˆã€‚")

    return profile_data

async def parse_user_head_data(head_json: dict) -> dict:
    """è§£æç”¨æˆ·å¤´éƒ¨APIçš„JSONæ•°æ®ã€‚"""
    data = head_json.get('data', {})
    ylz_tags = await safe_get(data, 'module', 'base', 'ylzTags', default=[])
    seller_credit, buyer_credit = {}, {}
    for tag in ylz_tags:
        if await safe_get(tag, 'attributes', 'role') == 'seller':
            seller_credit = {'level': await safe_get(tag, 'attributes', 'level'), 'text': tag.get('text')}
        elif await safe_get(tag, 'attributes', 'role') == 'buyer':
            buyer_credit = {'level': await safe_get(tag, 'attributes', 'level'), 'text': tag.get('text')}
    return {
        "å–å®¶æ˜µç§°": await safe_get(data, 'module', 'base', 'displayName'),
        "å–å®¶å¤´åƒé“¾æ¥": await safe_get(data, 'module', 'base', 'avatar', 'avatar'),
        "å–å®¶ä¸ªæ€§ç­¾å": await safe_get(data, 'module', 'base', 'introduction', default=''),
        "å–å®¶åœ¨å”®/å·²å”®å•†å“æ•°": await safe_get(data, 'module', 'tabs', 'item', 'number'),
        "å–å®¶æ”¶åˆ°çš„è¯„ä»·æ€»æ•°": await safe_get(data, 'module', 'tabs', 'rate', 'number'),
        "å–å®¶ä¿¡ç”¨ç­‰çº§": seller_credit.get('text', 'æš‚æ— '),
        "ä¹°å®¶ä¿¡ç”¨ç­‰çº§": buyer_credit.get('text', 'æš‚æ— ')
    }


async def parse_ratings_data(ratings_json: list) -> list:
    """è§£æè¯„ä»·åˆ—è¡¨APIçš„JSONæ•°æ®ã€‚"""
    parsed_list = []
    for card in ratings_json:
        data = await safe_get(card, 'cardData', default={})
        rate_tag = await safe_get(data, 'rateTagList', 0, 'text', default='æœªçŸ¥è§’è‰²')
        rate_type = await safe_get(data, 'rate')
        if rate_type == 1: rate_text = "å¥½è¯„"
        elif rate_type == 0: rate_text = "ä¸­è¯„"
        elif rate_type == -1: rate_text = "å·®è¯„"
        else: rate_text = "æœªçŸ¥"
        parsed_list.append({
            "è¯„ä»·ID": data.get('rateId'),
            "è¯„ä»·å†…å®¹": data.get('feedback'),
            "è¯„ä»·ç±»å‹": rate_text,
            "è¯„ä»·æ¥æºè§’è‰²": rate_tag,
            "è¯„ä»·è€…æ˜µç§°": data.get('raterUserNick'),
            "è¯„ä»·æ—¶é—´": data.get('gmtCreate'),
            "è¯„ä»·å›¾ç‰‡": await safe_get(data, 'pictCdnUrlList', default=[])
        })
    return parsed_list

async def safe_get(data, *keys, default="æš‚æ— "):
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸å€¼"""
    for key in keys:
        try:
            data = data[key]
        except (KeyError, TypeError, IndexError):
            return default
    return data

async def _parse_search_results_json(json_data: dict, source: str) -> list:
    """è§£ææœç´¢APIçš„JSONæ•°æ®ï¼Œè¿”å›åŸºç¡€å•†å“ä¿¡æ¯åˆ—è¡¨ã€‚"""
    page_data = []
    try:
        items = await safe_get(json_data, "data", "resultList", default=[])
        if not items:
            print(f"LOG: ({source}) APIå“åº”ä¸­æœªæ‰¾åˆ°å•†å“åˆ—è¡¨ (resultList)ã€‚")
            return []

        for item in items:
            main_data = await safe_get(item, "data", "item", "main", "exContent", default={})
            click_params = await safe_get(item, "data", "item", "main", "clickParam", "args", default={})

            title = await safe_get(main_data, "title", default="æœªçŸ¥æ ‡é¢˜")
            price_parts = await safe_get(main_data, "price", default=[])
            price = "".join([str(p.get("text", "")) for p in price_parts if isinstance(p, dict)]).replace("å½“å‰ä»·", "").strip() if isinstance(price_parts, list) else "ä»·æ ¼å¼‚å¸¸"
            if "ä¸‡" in price: price = f"Â¥{float(price.replace('Â¥', '').replace('ä¸‡', '')) * 10000:.0f}"
            area = await safe_get(main_data, "area", default="åœ°åŒºæœªçŸ¥")
            seller = await safe_get(main_data, "userNickName", default="åŒ¿åå–å®¶")
            raw_link = await safe_get(item, "data", "item", "main", "targetUrl", default="")
            image_url = await safe_get(main_data, "picUrl", default="")
            pub_time_ts = click_params.get("publishTime", "")
            item_id = await safe_get(main_data, "itemId", default="æœªçŸ¥ID")
            original_price = await safe_get(main_data, "oriPrice", default="æš‚æ— ")
            wants_count = await safe_get(click_params, "wantNum", default='NaN')


            tags = []
            if await safe_get(click_params, "tag") == "freeship":
                tags.append("åŒ…é‚®")
            r1_tags = await safe_get(main_data, "fishTags", "r1", "tagList", default=[])
            for tag_item in r1_tags:
                content = await safe_get(tag_item, "data", "content", default="")
                if "éªŒè´§å®" in content:
                    tags.append("éªŒè´§å®")

            page_data.append({
                "å•†å“æ ‡é¢˜": title,
                "å½“å‰å”®ä»·": price,
                "å•†å“åŸä»·": original_price,
                "â€œæƒ³è¦â€äººæ•°": wants_count,
                "å•†å“æ ‡ç­¾": tags,
                "å‘è´§åœ°åŒº": area,
                "å–å®¶æ˜µç§°": seller,
                "å•†å“é“¾æ¥": raw_link.replace("fleamarket://", "https://www.goofish.com/"),
                "å‘å¸ƒæ—¶é—´": datetime.fromtimestamp(int(pub_time_ts)/1000).strftime("%Y-%m-%d %H:%M") if pub_time_ts.isdigit() else "æœªçŸ¥æ—¶é—´",
                "å•†å“ID": item_id
            })
        print(f"LOG: ({source}) æˆåŠŸè§£æåˆ° {len(page_data)} æ¡å•†å“åŸºç¡€ä¿¡æ¯ã€‚")
        return page_data
    except Exception as e:
        print(f"LOG: ({source}) JSONæ•°æ®å¤„ç†å¼‚å¸¸: {str(e)}")
        return []

def format_registration_days(total_days: int) -> str:
    """
    å°†æ€»å¤©æ•°æ ¼å¼åŒ–ä¸ºâ€œXå¹´Yä¸ªæœˆâ€çš„å­—ç¬¦ä¸²ã€‚
    """
    if not isinstance(total_days, int) or total_days <= 0:
        return 'æœªçŸ¥'

    # ä½¿ç”¨æ›´ç²¾ç¡®çš„å¹³å‡å¤©æ•°
    DAYS_IN_YEAR = 365.25
    DAYS_IN_MONTH = DAYS_IN_YEAR / 12  # å¤§çº¦ 30.44

    # è®¡ç®—å¹´æ•°
    years = math.floor(total_days / DAYS_IN_YEAR)

    # è®¡ç®—å‰©ä½™å¤©æ•°
    remaining_days = total_days - (years * DAYS_IN_YEAR)

    # è®¡ç®—æœˆæ•°ï¼Œå››èˆäº”å…¥
    months = round(remaining_days / DAYS_IN_MONTH)

    # å¤„ç†è¿›ä½ï¼šå¦‚æœæœˆæ•°ç­‰äº12ï¼Œåˆ™å¹´æ•°åŠ 1ï¼Œæœˆæ•°å½’é›¶
    if months == 12:
        years += 1
        months = 0

    # æ„å»ºæœ€ç»ˆçš„è¾“å‡ºå­—ç¬¦ä¸²
    if years > 0 and months > 0:
        return f"æ¥é—²é±¼{years}å¹´{months}ä¸ªæœˆ"
    elif years > 0 and months == 0:
        return f"æ¥é—²é±¼{years}å¹´æ•´"
    elif years == 0 and months > 0:
        return f"æ¥é—²é±¼{months}ä¸ªæœˆ"
    else: # years == 0 and months == 0
        return "æ¥é—²é±¼ä¸è¶³ä¸€ä¸ªæœˆ"


# --- AIåˆ†æåŠé€šçŸ¥è¾…åŠ©å‡½æ•° (ä» ai_filter.py ç§»æ¤å¹¶å¼‚æ­¥åŒ–æ”¹é€ ) ---

def retry_on_failure(retries=3, delay=5):
    """
    ä¸€ä¸ªé€šç”¨çš„å¼‚æ­¥é‡è¯•è£…é¥°å™¨ï¼Œå¢åŠ äº†å¯¹HTTPé”™è¯¯çš„è¯¦ç»†æ—¥å¿—è®°å½•ã€‚
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return await func(*args, **kwargs)
                except (APIStatusError, HTTPError) as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥ï¼Œå‘ç”ŸHTTPé”™è¯¯ã€‚")
                    if hasattr(e, 'status_code'):
                        print(f"  - çŠ¶æ€ç  (Status Code): {e.status_code}")
                    if hasattr(e, 'response') and hasattr(e.response, 'text'):
                        response_text = e.response.text
                        print(
                            f"  - è¿”å›å€¼ (Response): {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
                except json.JSONDecodeError as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥: JSONè§£æé”™è¯¯ - {e}")
                except Exception as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥: {type(e).__name__} - {e}")

                if i < retries - 1:
                    print(f"å°†åœ¨ {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)

            print(f"å‡½æ•° {func.__name__} åœ¨ {retries} æ¬¡å°è¯•åå½»åº•å¤±è´¥ã€‚")
            return None
        return wrapper
    return decorator


@retry_on_failure(retries=2, delay=3)
async def _download_single_image(url, save_path):
    """ä¸€ä¸ªå¸¦é‡è¯•çš„å†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¼‚æ­¥ä¸‹è½½å•ä¸ªå›¾ç‰‡ã€‚"""
    loop = asyncio.get_running_loop()
    # ä½¿ç”¨ run_in_executor è¿è¡ŒåŒæ­¥çš„ requests ä»£ç ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
    response = await loop.run_in_executor(
        None,
        lambda: requests.get(url, headers=IMAGE_DOWNLOAD_HEADERS, timeout=20, stream=True)
    )
    response.raise_for_status()
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    return save_path


async def download_all_images(product_id, image_urls):
    """å¼‚æ­¥ä¸‹è½½ä¸€ä¸ªå•†å“çš„æ‰€æœ‰å›¾ç‰‡ã€‚å¦‚æœå›¾ç‰‡å·²å­˜åœ¨åˆ™è·³è¿‡ã€‚"""
    if not image_urls:
        return []

    urls = [url.strip() for url in image_urls if url.strip().startswith('http')]
    if not urls:
        return []

    saved_paths = []
    total_images = len(urls)
    for i, url in enumerate(urls):
        try:
            clean_url = url.split('.heic')[0] if '.heic' in url else url
            file_name_base = os.path.basename(clean_url).split('?')[0]
            file_name = f"product_{product_id}_{i + 1}_{file_name_base}"
            file_name = re.sub(r'[\\/*?:"<>|]', "", file_name)
            if not os.path.splitext(file_name)[1]:
                file_name += ".jpg"

            save_path = os.path.join(IMAGE_SAVE_DIR, file_name)

            if os.path.exists(save_path):
                print(f"   [å›¾ç‰‡] å›¾ç‰‡ {i + 1}/{total_images} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {os.path.basename(save_path)}")
                saved_paths.append(save_path)
                continue

            print(f"   [å›¾ç‰‡] æ­£åœ¨ä¸‹è½½å›¾ç‰‡ {i + 1}/{total_images}: {url}")
            if await _download_single_image(url, save_path):
                print(f"   [å›¾ç‰‡] å›¾ç‰‡ {i + 1}/{total_images} å·²æˆåŠŸä¸‹è½½åˆ°: {os.path.basename(save_path)}")
                saved_paths.append(save_path)
        except Exception as e:
            print(f"   [å›¾ç‰‡] å¤„ç†å›¾ç‰‡ {url} æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå·²è·³è¿‡æ­¤å›¾: {e}")

    return saved_paths


def encode_image_to_base64(image_path):
    """å°†æœ¬åœ°å›¾ç‰‡æ–‡ä»¶ç¼–ç ä¸º Base64 å­—ç¬¦ä¸²ã€‚"""
    if not image_path or not os.path.exists(image_path):
        return None
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"ç¼–ç å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None


@retry_on_failure(retries=3, delay=5)
async def send_ntfy_notification(product_data, reason):
    """å½“å‘ç°æ¨èå•†å“æ—¶ï¼Œå¼‚æ­¥å‘é€ä¸€ä¸ªé«˜ä¼˜å…ˆçº§çš„ ntfy.sh é€šçŸ¥ã€‚"""
    if not NTFY_TOPIC_URL:
        print("è­¦å‘Šï¼šæœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½® NTFY_TOPIC_URLï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return
    if not WX_BOT_URL:
        print("è­¦å‘Šï¼šæœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½® WX_BOT_URLï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return
    title = product_data.get('å•†å“æ ‡é¢˜', 'N/A')
    price = product_data.get('å½“å‰å”®ä»·', 'N/A')
    link = product_data.get('å•†å“é“¾æ¥', '#')
    if PCURL_TO_MOBILE:
        mobile_link = convert_goofish_link(link)
        message = f"ä»·æ ¼: {price}\nåŸå› : {reason}\næ‰‹æœºç«¯é“¾æ¥: {mobile_link}\nç”µè„‘ç«¯é“¾æ¥: {link}"
    else:
        message = f"ä»·æ ¼: {price}\nåŸå› : {reason}\né“¾æ¥: {link}"

    notification_title = f"ğŸš¨ æ–°æ¨è! {title[:30]}..."

    try:
        print(f"   -> æ­£åœ¨å‘é€ ntfy é€šçŸ¥åˆ°: {NTFY_TOPIC_URL}")
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None,
            lambda: requests.post(
                NTFY_TOPIC_URL,
                data=message.encode('utf-8'),
                headers={
                    "Title": notification_title.encode('utf-8'),
                    "Priority": "urgent",
                    "Tags": "bell,vibration"
                },
                timeout=10
            )
        )
        print("   -> é€šçŸ¥å‘é€æˆåŠŸã€‚")
    except Exception as e:
        print(f"   -> å‘é€ ntfy é€šçŸ¥å¤±è´¥: {e}")
        raise

    # ä¼ä¸šå¾®ä¿¡æ–‡æœ¬æ¶ˆæ¯çš„ payload æ ¼å¼
    payload = {
        "msgtype": "text",
        "text": {
            "content": f"{notification_title}\n{message}"
        }
    }

    try:
        print(f"   -> æ­£åœ¨å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥åˆ°: {WX_BOT_URL}")
        # è®¾ç½®æ­£ç¡®çš„ Content-Type ä¸º application/json
        headers = {
            "Content-Type": "application/json"
        }
        # ä½¿ç”¨ json å‚æ•°ç›´æ¥å‘é€å­—å…¸ï¼Œrequests ä¼šè‡ªåŠ¨å¤„ç†ç¼–ç å’Œ Content-Type
        response = requests.post(
            WX_BOT_URL,
            json=payload,
            headers=headers,
            timeout=10
        )
        response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç æ˜¯å¦ä¸ºé”™è¯¯ (å¦‚4xxæˆ–5xx)
        result = response.json()
        print(f"   -> é€šçŸ¥å‘é€æˆåŠŸã€‚å“åº”: {result}")
    except requests.exceptions.RequestException as e:
        print(f"   -> å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¤±è´¥: {e}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ä¸Šå±‚å¯ä»¥æ•è·
    except Exception as e:
        print(f"   -> å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise

@retry_on_failure(retries=5, delay=10)
async def get_ai_analysis(product_data, image_paths=None, prompt_text=""):
    """å°†å®Œæ•´çš„å•†å“JSONæ•°æ®å’Œæ‰€æœ‰å›¾ç‰‡å‘é€ç»™ AI è¿›è¡Œåˆ†æï¼ˆå¼‚æ­¥ï¼‰ã€‚"""
    item_info = product_data.get('å•†å“ä¿¡æ¯', {})
    product_id = item_info.get('å•†å“ID', 'N/A')

    print(f"\n   [AIåˆ†æ] å¼€å§‹åˆ†æå•†å“ #{product_id} (å« {len(image_paths or [])} å¼ å›¾ç‰‡)...")
    print(f"   [AIåˆ†æ] æ ‡é¢˜: {item_info.get('å•†å“æ ‡é¢˜', 'æ— ')}")

    if not prompt_text:
        print("   [AIåˆ†æ] é”™è¯¯ï¼šæœªæä¾›AIåˆ†ææ‰€éœ€çš„promptæ–‡æœ¬ã€‚")
        return None

    product_details_json = json.dumps(product_data, ensure_ascii=False, indent=2)
    system_prompt = prompt_text

    combined_text_prompt = f"""{system_prompt}

è¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œæˆ‘çš„è¦æ±‚ï¼Œåˆ†æä»¥ä¸‹å®Œæ•´çš„å•†å“JSONæ•°æ®ï¼š

```json
    {product_details_json}
"""
    user_content_list = [{"type": "text", "text": combined_text_prompt}]

    if image_paths:
        for path in image_paths:
            base64_image = encode_image_to_base64(path)
            if base64_image:
                user_content_list.append(
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})

    messages = [{"role": "user", "content": user_content_list}]

    response = await client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        response_format={"type": "json_object"}
    )

    ai_response_content = response.choices[0].message.content

    try:
        return json.loads(ai_response_content)
    except json.JSONDecodeError as e:
        print("---!!! AI RESPONSE PARSING FAILED (JSONDecodeError) !!!---")
        print(f"åŸå§‹è¿”å›å€¼ (Raw response from AI):\n---\n{ai_response_content}\n---")
        raise e


async def scrape_xianyu(task_config: dict, debug_limit: int = 0):
    """
    ã€æ ¸å¿ƒæ‰§è¡Œå™¨ã€‘
    æ ¹æ®å•ä¸ªä»»åŠ¡é…ç½®ï¼Œå¼‚æ­¥çˆ¬å–é—²é±¼å•†å“æ•°æ®ï¼Œå¹¶å¯¹æ¯ä¸ªæ–°å‘ç°çš„å•†å“è¿›è¡Œå®æ—¶çš„ã€ç‹¬ç«‹çš„AIåˆ†æå’Œé€šçŸ¥ã€‚
    """
    keyword = task_config['keyword']
    max_pages = task_config.get('max_pages', 1)
    personal_only = task_config.get('personal_only', False)
    min_price = task_config.get('min_price')
    max_price = task_config.get('max_price')
    ai_prompt_text = task_config.get('ai_prompt_text', '')

    processed_item_count = 0
    stop_scraping = False

    processed_links = set()
    output_filename = f"{keyword.replace(' ', '_')}_full_data.jsonl"
    if os.path.exists(output_filename):
        print(f"LOG: å‘ç°å·²å­˜åœ¨æ–‡ä»¶ {output_filename}ï¼Œæ­£åœ¨åŠ è½½å†å²è®°å½•ä»¥å»é‡...")
        try:
            with open(output_filename, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        link = record.get('å•†å“ä¿¡æ¯', {}).get('å•†å“é“¾æ¥', '')
                        if link:
                            processed_links.add(get_link_unique_key(link))
                    except json.JSONDecodeError:
                        print(f"   [è­¦å‘Š] æ–‡ä»¶ä¸­æœ‰ä¸€è¡Œæ— æ³•è§£æä¸ºJSONï¼Œå·²è·³è¿‡ã€‚")
            print(f"LOG: åŠ è½½å®Œæˆï¼Œå·²è®°å½• {len(processed_links)} ä¸ªå·²å¤„ç†è¿‡çš„å•†å“ã€‚")
        except IOError as e:
            print(f"   [è­¦å‘Š] è¯»å–å†å²æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print(f"LOG: è¾“å‡ºæ–‡ä»¶ {output_filename} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=RUN_HEADLESS)
        context = await browser.new_context(storage_state=STATE_FILE, user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3")
        page = await context.new_page()

        try:
            print("LOG: æ­¥éª¤ 1 - ç›´æ¥å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µ...")
            # ä½¿ç”¨ 'q' å‚æ•°æ„å»ºæ­£ç¡®çš„æœç´¢URLï¼Œå¹¶è¿›è¡ŒURLç¼–ç 
            params = {'q': keyword}
            search_url = f"https://www.goofish.com/search?{urlencode(params)}"
            print(f"   -> ç›®æ ‡URL: {search_url}")

            # ä½¿ç”¨ expect_response åœ¨å¯¼èˆªçš„åŒæ—¶æ•è·åˆå§‹æœç´¢çš„APIæ•°æ®
            async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                await page.goto(search_url, wait_until="domcontentloaded", timeout=60000)

            initial_response = await response_info.value

            # ç­‰å¾…é¡µé¢åŠ è½½å‡ºå…³é”®ç­›é€‰å…ƒç´ ï¼Œä»¥ç¡®è®¤å·²æˆåŠŸè¿›å…¥æœç´¢ç»“æœé¡µ
            await page.wait_for_selector('text=æ–°å‘å¸ƒ', timeout=15000)

            # --- æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯å¼¹çª— ---
            baxia_dialog = page.locator("div.baxia-dialog-mask")
            if await baxia_dialog.is_visible(timeout=2000): # çŸ­æš‚ç­‰å¾…æ£€æŸ¥
                print("\n==================== CRITICAL BLOCK DETECTED ====================")
                print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯å¼¹çª— (baxia-dialog)ï¼Œæ— æ³•ç»§ç»­æ“ä½œã€‚")
                print("è¿™é€šå¸¸æ˜¯å› ä¸ºæ“ä½œè¿‡äºé¢‘ç¹æˆ–è¢«è¯†åˆ«ä¸ºæœºå™¨äººã€‚")
                print("å»ºè®®ï¼š")
                print("1. åœæ­¢è„šæœ¬ä¸€æ®µæ—¶é—´å†è¯•ã€‚")
                print("2. (æ¨è) åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® RUN_HEADLESS=falseï¼Œä»¥éæ— å¤´æ¨¡å¼è¿è¡Œï¼Œè¿™æœ‰åŠ©äºç»•è¿‡æ£€æµ‹ã€‚")
                print(f"ä»»åŠ¡ '{keyword}' å°†åœ¨æ­¤å¤„ä¸­æ­¢ã€‚")
                print("===================================================================")
                await browser.close()
                return processed_item_count
            # --- ç»“æŸæ–°å¢ ---

            try:
                await page.click("div[class*='closeIconBg']", timeout=3000)
                print("LOG: å·²å…³é—­å¹¿å‘Šå¼¹çª—ã€‚")
            except PlaywrightTimeoutError:
                print("LOG: æœªæ£€æµ‹åˆ°å¹¿å‘Šå¼¹çª—ã€‚")

            final_response = None
            print("\nLOG: æ­¥éª¤ 2 - åº”ç”¨ç­›é€‰æ¡ä»¶...")
            await page.click('text=æ–°å‘å¸ƒ')
            await random_sleep(2, 4) # åŸæ¥æ˜¯ (1.5, 2.5)
            async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                await page.click('text=æœ€æ–°')
                # --- ä¿®æ”¹: å¢åŠ æ’åºåçš„ç­‰å¾…æ—¶é—´ ---
                await random_sleep(4, 7) # åŸæ¥æ˜¯ (3, 5)
            final_response = await response_info.value

            if personal_only:
                async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                    await page.click('text=ä¸ªäººé—²ç½®')
                    # --- ä¿®æ”¹: å°†å›ºå®šç­‰å¾…æ”¹ä¸ºéšæœºç­‰å¾…ï¼Œå¹¶åŠ é•¿ ---
                    await random_sleep(4, 6) # åŸæ¥æ˜¯ asyncio.sleep(5)
                final_response = await response_info.value

            if min_price or max_price:
                price_container = page.locator('div[class*="search-price-input-container"]').first
                if await price_container.is_visible():
                    if min_price:
                        await price_container.get_by_placeholder("Â¥").first.fill(min_price)
                        # --- ä¿®æ”¹: å°†å›ºå®šç­‰å¾…æ”¹ä¸ºéšæœºç­‰å¾… ---
                        await random_sleep(1, 2.5) # åŸæ¥æ˜¯ asyncio.sleep(5)
                    if max_price:
                        await price_container.get_by_placeholder("Â¥").nth(1).fill(max_price)
                        # --- ä¿®æ”¹: å°†å›ºå®šç­‰å¾…æ”¹ä¸ºéšæœºç­‰å¾… ---
                        await random_sleep(1, 2.5) # åŸæ¥æ˜¯ asyncio.sleep(5)

                    async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                        await page.keyboard.press('Tab')
                        # --- ä¿®æ”¹: å¢åŠ ç¡®è®¤ä»·æ ¼åçš„ç­‰å¾…æ—¶é—´ ---
                        await random_sleep(4, 7) # åŸæ¥æ˜¯ asyncio.sleep(5)
                    final_response = await response_info.value
                else:
                    print("LOG: è­¦å‘Š - æœªæ‰¾åˆ°ä»·æ ¼è¾“å…¥å®¹å™¨ã€‚")

            print("\nLOG: æ‰€æœ‰ç­›é€‰å·²å®Œæˆï¼Œå¼€å§‹å¤„ç†å•†å“åˆ—è¡¨...")

            current_response = final_response if final_response and final_response.ok else initial_response
            for page_num in range(1, max_pages + 1):
                if stop_scraping: break
                print(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {page_num}/{max_pages} é¡µ ---")

                if page_num > 1:
                    next_btn = page.locator("[class*='search-pagination-arrow-right']:not([disabled])")
                    if not await next_btn.count():
                        print("LOG: æœªæ‰¾åˆ°å¯ç”¨çš„â€œä¸‹ä¸€é¡µâ€æŒ‰é’®ï¼Œåœæ­¢ç¿»é¡µã€‚")
                        break
                    try:
                        async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                            await next_btn.click()
                            # --- ä¿®æ”¹: å¢åŠ ç¿»é¡µåçš„ç­‰å¾…æ—¶é—´ ---
                            await random_sleep(5, 8) # åŸæ¥æ˜¯ (1.5, 3.5)
                        current_response = await response_info.value
                    except PlaywrightTimeoutError:
                        print(f"LOG: ç¿»é¡µåˆ°ç¬¬ {page_num} é¡µè¶…æ—¶ã€‚")
                        break

                if not (current_response and current_response.ok):
                    print(f"LOG: ç¬¬ {page_num} é¡µå“åº”æ— æ•ˆï¼Œè·³è¿‡ã€‚")
                    continue

                basic_items = await _parse_search_results_json(await current_response.json(), f"ç¬¬ {page_num} é¡µ")
                if not basic_items: break

                total_items_on_page = len(basic_items)
                for i, item_data in enumerate(basic_items, 1):
                    if debug_limit > 0 and processed_item_count >= debug_limit:
                        print(f"LOG: å·²è¾¾åˆ°è°ƒè¯•ä¸Šé™ ({debug_limit})ï¼Œåœæ­¢è·å–æ–°å•†å“ã€‚")
                        stop_scraping = True
                        break

                    unique_key = get_link_unique_key(item_data["å•†å“é“¾æ¥"])
                    if unique_key in processed_links:
                        print(f"   -> [é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å•†å“ '{item_data['å•†å“æ ‡é¢˜'][:20]}...' å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                        continue

                    print(f"-> [é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å‘ç°æ–°å•†å“ï¼Œè·å–è¯¦æƒ…: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                    # --- ä¿®æ”¹: è®¿é—®è¯¦æƒ…é¡µå‰çš„ç­‰å¾…æ—¶é—´ï¼Œæ¨¡æ‹Ÿç”¨æˆ·åœ¨åˆ—è¡¨é¡µä¸Šçœ‹äº†ä¸€ä¼šå„¿ ---
                    await random_sleep(3, 6) # åŸæ¥æ˜¯ (2, 4)

                    detail_page = await context.new_page()
                    try:
                        async with detail_page.expect_response(lambda r: DETAIL_API_URL_PATTERN in r.url, timeout=25000) as detail_info:
                            await detail_page.goto(item_data["å•†å“é“¾æ¥"], wait_until="domcontentloaded", timeout=25000)

                        detail_response = await detail_info.value
                        if detail_response.ok:
                            detail_json = await detail_response.json()

                            ret_string = str(await safe_get(detail_json, 'ret', default=[]))
                            if "FAIL_SYS_USER_VALIDATE" in ret_string:
                                print("\n==================== CRITICAL BLOCK DETECTED ====================")
                                print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯ (FAIL_SYS_USER_VALIDATE)ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
                                long_sleep_duration = random.randint(300, 600)
                                print(f"ä¸ºé¿å…è´¦æˆ·é£é™©ï¼Œå°†æ‰§è¡Œä¸€æ¬¡é•¿æ—¶é—´ä¼‘çœ  ({long_sleep_duration} ç§’) åå†é€€å‡º...")
                                await asyncio.sleep(long_sleep_duration)
                                print("é•¿æ—¶é—´ä¼‘çœ ç»“æŸï¼Œç°åœ¨å°†å®‰å…¨é€€å‡ºã€‚")
                                print("===================================================================")
                                stop_scraping = True
                                break

                            # è§£æå•†å“è¯¦æƒ…æ•°æ®å¹¶æ›´æ–° item_data
                            item_do = await safe_get(detail_json, 'data', 'itemDO', default={})
                            seller_do = await safe_get(detail_json, 'data', 'sellerDO', default={})

                            reg_days_raw = await safe_get(seller_do, 'userRegDay', default=0)
                            registration_duration_text = format_registration_days(reg_days_raw)

                            # --- START: æ–°å¢ä»£ç å— ---

                            # 1. æå–å–å®¶çš„èŠéº»ä¿¡ç”¨ä¿¡æ¯
                            zhima_credit_text = await safe_get(seller_do, 'zhimaLevelInfo', 'levelName')

                            # 2. æå–è¯¥å•†å“çš„å®Œæ•´å›¾ç‰‡åˆ—è¡¨
                            image_infos = await safe_get(item_do, 'imageInfos', default=[])
                            if image_infos:
                                # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è·å–æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡URL
                                all_image_urls = [img.get('url') for img in image_infos if img.get('url')]
                                if all_image_urls:
                                    # ç”¨æ–°çš„å­—æ®µå­˜å‚¨å›¾ç‰‡åˆ—è¡¨ï¼Œæ›¿æ¢æ‰æ—§çš„å•ä¸ªé“¾æ¥
                                    item_data['å•†å“å›¾ç‰‡åˆ—è¡¨'] = all_image_urls
                                    # (å¯é€‰) ä»ç„¶ä¿ç•™ä¸»å›¾é“¾æ¥ï¼Œä»¥é˜²ä¸‡ä¸€
                                    item_data['å•†å“ä¸»å›¾é“¾æ¥'] = all_image_urls[0]

                            # --- END: æ–°å¢ä»£ç å— ---
                            item_data['â€œæƒ³è¦â€äººæ•°'] = await safe_get(item_do, 'wantCnt', default=item_data.get('â€œæƒ³è¦â€äººæ•°', 'NaN'))
                            item_data['æµè§ˆé‡'] = await safe_get(item_do, 'browseCnt', default='-')
                            # ...[æ­¤å¤„å¯æ·»åŠ æ›´å¤šä»è¯¦æƒ…é¡µè§£æå‡ºçš„å•†å“ä¿¡æ¯]...

                            # è°ƒç”¨æ ¸å¿ƒå‡½æ•°é‡‡é›†å–å®¶ä¿¡æ¯
                            user_profile_data = {}
                            user_id = await safe_get(seller_do, 'sellerId')
                            if user_id:
                                # æ–°çš„ã€é«˜æ•ˆçš„è°ƒç”¨æ–¹å¼:
                                user_profile_data = await scrape_user_profile(context, str(user_id))
                            else:
                                print("   [è­¦å‘Š] æœªèƒ½ä»è¯¦æƒ…APIä¸­è·å–åˆ°å–å®¶IDã€‚")
                            user_profile_data['å–å®¶èŠéº»ä¿¡ç”¨'] = zhima_credit_text
                            user_profile_data['å–å®¶æ³¨å†Œæ—¶é•¿'] = registration_duration_text

                            # æ„å»ºåŸºç¡€è®°å½•
                            final_record = {
                                "çˆ¬å–æ—¶é—´": datetime.now().isoformat(),
                                "æœç´¢å…³é”®å­—": keyword,
                                "ä»»åŠ¡åç§°": task_config.get('task_name', 'Untitled Task'),
                                "å•†å“ä¿¡æ¯": item_data,
                                "å–å®¶ä¿¡æ¯": user_profile_data
                            }

                            # --- START: Real-time AI Analysis & Notification ---
                            print(f"   -> å¼€å§‹å¯¹å•†å“ #{item_data['å•†å“ID']} è¿›è¡Œå®æ—¶AIåˆ†æ...")
                            # 1. Download images
                            image_urls = item_data.get('å•†å“å›¾ç‰‡åˆ—è¡¨', [])
                            downloaded_image_paths = await download_all_images(item_data['å•†å“ID'], image_urls)

                            # 2. Get AI analysis
                            ai_analysis_result = None
                            if ai_prompt_text:
                                try:
                                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°†æ•´ä¸ªè®°å½•ä¼ ç»™AIï¼Œè®©å®ƒæ‹¥æœ‰æœ€å…¨çš„ä¸Šä¸‹æ–‡
                                    ai_analysis_result = await get_ai_analysis(final_record, downloaded_image_paths, prompt_text=ai_prompt_text)
                                    if ai_analysis_result:
                                        final_record['ai_analysis'] = ai_analysis_result
                                        print(f"   -> AIåˆ†æå®Œæˆã€‚æ¨èçŠ¶æ€: {ai_analysis_result.get('is_recommended')}")
                                    else:
                                        final_record['ai_analysis'] = {'error': 'AI analysis returned None after retries.'}
                                except Exception as e:
                                    print(f"   -> AIåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                                    final_record['ai_analysis'] = {'error': str(e)}
                            else:
                                print("   -> ä»»åŠ¡æœªé…ç½®AI promptï¼Œè·³è¿‡åˆ†æã€‚")

                            # 3. Send notification if recommended
                            if ai_analysis_result and ai_analysis_result.get('is_recommended'):
                                print(f"   -> å•†å“è¢«AIæ¨èï¼Œå‡†å¤‡å‘é€é€šçŸ¥...")
                                await send_ntfy_notification(item_data, ai_analysis_result.get("reason", "æ— "))
                            # --- END: Real-time AI Analysis & Notification ---

                            # 4. ä¿å­˜åŒ…å«AIç»“æœçš„å®Œæ•´è®°å½•
                            await save_to_jsonl(final_record, keyword)

                            processed_links.add(unique_key)
                            processed_item_count += 1
                            print(f"   -> å•†å“å¤„ç†æµç¨‹å®Œæ¯•ã€‚ç´¯è®¡å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“ã€‚")

                            # --- ä¿®æ”¹: å¢åŠ å•ä¸ªå•†å“å¤„ç†åçš„ä¸»è¦å»¶è¿Ÿ ---
                            print("   [åçˆ¬] æ‰§è¡Œä¸€æ¬¡ä¸»è¦çš„éšæœºå»¶è¿Ÿä»¥æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆé—´éš”...")
                            await random_sleep(15, 30) # åŸæ¥æ˜¯ (8, 15)ï¼Œè¿™æ˜¯æœ€é‡è¦çš„ä¿®æ”¹ä¹‹ä¸€

                    except PlaywrightTimeoutError:
                        print(f"   é”™è¯¯: è®¿é—®å•†å“è¯¦æƒ…é¡µæˆ–ç­‰å¾…APIå“åº”è¶…æ—¶ã€‚")
                    except Exception as e:
                        print(f"   é”™è¯¯: å¤„ç†å•†å“è¯¦æƒ…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    finally:
                        await detail_page.close()
                        # --- ä¿®æ”¹: å¢åŠ å…³é—­é¡µé¢åçš„çŸ­æš‚æ•´ç†æ—¶é—´ ---
                        await random_sleep(2, 4) # åŸæ¥æ˜¯ (1, 2.5)

                # --- æ–°å¢: åœ¨å¤„ç†å®Œä¸€é¡µæ‰€æœ‰å•†å“åï¼Œç¿»é¡µå‰ï¼Œå¢åŠ ä¸€ä¸ªæ›´é•¿çš„â€œä¼‘æ¯â€æ—¶é—´ ---
                if not stop_scraping and page_num < max_pages:
                    print(f"--- ç¬¬ {page_num} é¡µå¤„ç†å®Œæ¯•ï¼Œå‡†å¤‡ç¿»é¡µã€‚æ‰§è¡Œä¸€æ¬¡é¡µé¢é—´çš„é•¿æ—¶ä¼‘æ¯... ---")
                    await random_sleep(25, 50)

        except PlaywrightTimeoutError as e:
            print(f"\næ“ä½œè¶…æ—¶é”™è¯¯: é¡µé¢å…ƒç´ æˆ–ç½‘ç»œå“åº”æœªåœ¨è§„å®šæ—¶é—´å†…å‡ºç°ã€‚\n{e}")
        except Exception as e:
            print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        finally:
            print("\nLOG: ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œæµè§ˆå™¨å°†åœ¨5ç§’åè‡ªåŠ¨å…³é—­...")
            await asyncio.sleep(5)
            if debug_limit:
                input("æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            await browser.close()

    return processed_item_count

async def main():
    parser = argparse.ArgumentParser(
        description="é—²é±¼å•†å“ç›‘æ§è„šæœ¬ï¼Œæ”¯æŒå¤šä»»åŠ¡é…ç½®å’Œå®æ—¶AIåˆ†æã€‚",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¿è¡Œ config.json ä¸­å®šä¹‰çš„æ‰€æœ‰ä»»åŠ¡
  python spider_v2.py

  # è°ƒè¯•æ¨¡å¼: è¿è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä½†æ¯ä¸ªä»»åŠ¡åªå¤„ç†å‰3ä¸ªæ–°å‘ç°çš„å•†å“
  python spider_v2.py --debug-limit 3
""",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--debug-limit", type=int, default=0, help="è°ƒè¯•æ¨¡å¼ï¼šæ¯ä¸ªä»»åŠ¡ä»…å¤„ç†å‰ N ä¸ªæ–°å•†å“ï¼ˆ0 è¡¨ç¤ºæ— é™åˆ¶ï¼‰")
    parser.add_argument("--config", type=str, default="config.json", help="æŒ‡å®šä»»åŠ¡é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸º config.jsonï¼‰")
    args = parser.parse_args()

    if not os.path.exists(STATE_FILE):
        sys.exit(f"é”™è¯¯: ç™»å½•çŠ¶æ€æ–‡ä»¶ '{STATE_FILE}' ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ login.py ç”Ÿæˆã€‚")

    if not os.path.exists(args.config):
        sys.exit(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{args.config}' ä¸å­˜åœ¨ã€‚")

    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            tasks_config = json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        sys.exit(f"é”™è¯¯: è¯»å–æˆ–è§£æé…ç½®æ–‡ä»¶ '{args.config}' å¤±è´¥: {e}")

    # è¯»å–æ‰€æœ‰promptæ–‡ä»¶å†…å®¹
    for task in tasks_config:
        if task.get("enabled", False) and task.get("ai_prompt_base_file") and task.get("ai_prompt_criteria_file"):
            try:
                with open(task["ai_prompt_base_file"], 'r', encoding='utf-8') as f_base:
                    base_prompt = f_base.read()
                with open(task["ai_prompt_criteria_file"], 'r', encoding='utf-8') as f_criteria:
                    criteria_text = f_criteria.read()
                
                # åŠ¨æ€ç»„åˆæˆæœ€ç»ˆçš„Prompt
                task['ai_prompt_text'] = base_prompt.replace("{{CRITERIA_SECTION}}", criteria_text)

            except FileNotFoundError as e:
                print(f"è­¦å‘Š: ä»»åŠ¡ '{task['task_name']}' çš„promptæ–‡ä»¶ç¼ºå¤±: {e}ï¼Œè¯¥ä»»åŠ¡çš„AIåˆ†æå°†è¢«è·³è¿‡ã€‚")
                task['ai_prompt_text'] = ""
        elif task.get("enabled", False) and task.get("ai_prompt_file"):
            try:
                with open(task["ai_prompt_file"], 'r', encoding='utf-8') as f:
                    task['ai_prompt_text'] = f.read()
            except FileNotFoundError:
                print(f"è­¦å‘Š: ä»»åŠ¡ '{task['task_name']}' çš„promptæ–‡ä»¶ '{task['ai_prompt_file']}' æœªæ‰¾åˆ°ï¼Œè¯¥ä»»åŠ¡çš„AIåˆ†æå°†è¢«è·³è¿‡ã€‚")
                task['ai_prompt_text'] = ""

    print("\n--- å¼€å§‹æ‰§è¡Œç›‘æ§ä»»åŠ¡ ---")
    if args.debug_limit > 0:
        print(f"** è°ƒè¯•æ¨¡å¼å·²æ¿€æ´»ï¼Œæ¯ä¸ªä»»åŠ¡æœ€å¤šå¤„ç† {args.debug_limit} ä¸ªæ–°å•†å“ **")
    print("--------------------")

    active_task_configs = [task for task in tasks_config if task.get("enabled", False)]
    if not active_task_configs:
        print("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ä¸ºæ¯ä¸ªå¯ç”¨çš„ä»»åŠ¡åˆ›å»ºä¸€ä¸ªå¼‚æ­¥æ‰§è¡Œåç¨‹
    coroutines = []
    for task_conf in active_task_configs:
        print(f"-> ä»»åŠ¡ '{task_conf['task_name']}' å·²åŠ å…¥æ‰§è¡Œé˜Ÿåˆ—ã€‚")
        coroutines.append(scrape_xianyu(task_config=task_conf, debug_limit=args.debug_limit))

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*coroutines, return_exceptions=True)

    print("\n--- æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯• ---")
    for i, result in enumerate(results):
        task_name = active_task_configs[i]['task_name']
        if isinstance(result, Exception):
            print(f"ä»»åŠ¡ '{task_name}' å› å¼‚å¸¸è€Œç»ˆæ­¢: {result}")
        else:
            print(f"ä»»åŠ¡ '{task_name}' æ­£å¸¸ç»“æŸï¼Œæœ¬æ¬¡è¿è¡Œå…±å¤„ç†äº† {result} ä¸ªæ–°å•†å“ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
